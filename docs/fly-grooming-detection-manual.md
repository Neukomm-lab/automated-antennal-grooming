# Fly Grooming - Automatic detection

## Installation

_Try first the new CUDA and new pytorch versions!_

- install miniconda
- install python (check version)
- check Nvidia card (computing number and model)
- update Nvidia driver
- install cuda
- install pytorch
- install remaining libraries (check environment requirements.txt)

from: https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#building-identical-conda-environments

- Install as many requirements as possible with conda then use pip.
- Pip should be run with --upgrade-strategy only-if-needed (the default).


## Connection to remote computer (notebook)

computer name: **DNF32758.ad.unil.ch**

## Overall design

> Two networks interact to automatically detect grooming (head grooming) in flies

`Network 1` is a fully convolutional network trained to detect the position of the fly's head. `Network 2` is a convolutional network with a pretrained ResNet50 backbone trained to recognized 'grooming' images.

## Network 1

### Preprocessing

input frames are **704 x 580 px**. The train set is generated using a custom made imageJ macro. Briefly, clicking on the head of the fly (center/front) generates a circle (radius = 42 px) covering the head and the space in front of the head (where the legs are placed during grooming episodes). The frames are extracted from the original movies (no cropping/resizing). This process generates two sets of data: frames + labels (i.e. 8 bit masks).
> output:
> Frames
> Masks (8 - bit labels)
> Coordinates (csv file. legacy output, not necessary...)
> log file (id_movie, frame number, ROI coordinates, ROI_size, if resize applied,	tool used to trace the ROI)
> metadata file (info about cropping, ROI size ... this is used by the macro to enable multi-session labelling)

### Training / Validation

input frames and masks are loaded by a custom dataset loader (_mouse_dataset.py_). Both input frames and masks are resized to **256 x 210 px**. A series of transformations (flip-H, flip-V, Blur, Noise) is applied to the frames during training. By default all frames are rescaled to 0-1 (i.e. the frame is divided by 255). The model with the best validation accuracy must be saved manually.

**>>> Update (2020)**: the model is saved automatically based on the accuracy (expressed as minimal average distance between target and predicted center of the score map).

Once trained the network outputs a score-map (i.e. pixel-level predictions). A hard-coded threshold (= 150) is used to define the ROI of the score-map (i.e. every pixel whose value >= 150 is part of the foreground). The centroid (approximated by the max value) of the score map is used as estimate for the position of the head on the frame.
> **output**:
> Trained model/weights

### Inference
The input is the original video file. Each frame is resized to 256 x 210 px (to match the training's resolution). No cropping is applied.
> **output**:
> Movie file with predicted head position
> CSV file with coordinates of the score map's centroid

**>>> Update (2021: 2021-03-19)**

### Test

A new script to test the performance of the trained network 1 on the test-set.
the latest (2021-03-19) accuracy is:

> Number of samples in TEST set: 196
> Average distance (+/- STD): 2.73 +/- 1.12 px
> Pixel accuracy threshold: 8 px
> Accuracy:  99.49 %

## Network 2

### Preprocessing

The train set is generated by resizing the original frames to **256 X 210 px**. A custom made imageJ macro is used to crop the frame at the head level (click on forehead to crop the frame to a **square image: 84 x 84 px**) and a rolling sum of 10 consecutive frames is applied to generate the final dataset.

- output: Images (sum of 10 consecutive frames)

**>>> Update (2021)**: ten consecutive frames are too much to detect exclusively the antennae's grooming. The algorithm has been changed to produce a stack of 3 frames and to project on the `z-axis` the _standard deviation_ of the frames (not the sum!).T his improves the accuracy of the grooming (on the antennae).
The preprocessing is done entirely in python with : `build\src\train\network-2\preprocessing\generate_dataset_from_labels.py`


### Training

The whole dataset is split into separate folders: `train > [grooming] / [no-grooming]`, and  `validation > [grooming] / [no-grooming]`
Pytorch's data loader is used to automatically assign labels (by means of folders' name) to the frames.
> output:
> Trained model/weights

### Inference

The network takes a movie and a csv file storing the coordinates of the head position (output from Network 1) as input.
the network converts each input frame as a sum of 10 consecutive frames, then processes it. The frame is resized to 256 x 210 px, then cropped to 84 x 84 px centered on the head position (from the input coordinates csv file)



***
***

## Notes

### 2020-11-10
**TODO**: Restructure train folder: add validation and test sets. the goal is to get rid of the random sampling of the validation set. the set should come from a different sample of frames. this would make the network  more robust.
***

### 2021-02-26
installed the two networks on Maria's laptop. tested network 1: it works! Network 2 has yet to be tested. Network 2 looks for file in a hard-coded location (make it so that it looks for files in the appropriate folder using a metadata `json` file)
***

### 2021-03-19
- **Wing reflection issue**: in new videos the arena is moved from the central position and there is a strong light reflection on the wings that trips up the recognition of the head
- moved the `train/val/test` folder to `D:` (cleaner structure in the `build` folder)
- Added extra samples in the train, validation and test folder to cope with the wing-reflection issue
***

### 2021-04-15
- implement a 3D CNN to replace the ResNet(?)

things to do:

1. take video (with labels from Ethovision)
2. process it so it is cropped to the head
3. take cropped video and labels to train
***

### 2021-05-10
Continue as planned: use RESNET for classification (even if it is dead slow)
as soon as the code is shipped, if/when time allows, I can try to implement the 3D CNN hoping to get faster inference
***

### 2021-05-14

Things to do:

- cumulative processing time for network-2 to print out at the end
- once videos are processed by network 2, transfer them in a `done` folder (using the date-time-hour of processing-start).
- output a metadata summary with name of processed files and the date-time (time-stamp)
***

### 2022-06-17
The new chamber (fixed layout) is good for the detection of the LED (we may use a python script to detect the on/off pattern). However, accuracy of network one seems to be a bit decreased (only assessed by eye, no formal analysis run). I will start training network-1 with frames taken from the new videos.

The plan is to shape-up the manual with the latest improvements and technical descriptions
***

### 2022-06-30
Add more training sample both for (network 1 and 2) with fly sitting on the side (too many false positive when on the side)
***

### 2022-07-25
Meeting with Lukas and Maria:
- variable frame rate issue: the laptop drops frames, try with one of the workstations of the NeuroBAU
- extract the onset/offset of the light from the video files
- find the overall threshold for P(grooming) so to minimize false positives (will try optimizing the F1-score)
- 
***

### 2022-08-04
New script (`build\src\consolidate_raw_data.py`) for merging together data from the head coordinates [speed, x, y] and the automated grooming info [timestamp, p(grooming)].
Network-1 is now detects the red light onset/offset. This capability depends entirely on the position of the red light: any movement of the red light from the field of view may cause the system to fail (i.e. the light is not detected)
***

### 2022-08-09
Starting the new training of Network-2 incorporated 2 new videos (`4_1` and `6_1`) and cleaned up the training set for grooming examples. Some frames showed grooming while the animal was walking, this may lead to false positives (walking classified as grooming). Overall accuracy for network-2 went up from 92% to ~94%. Still have problems with _one-leg-grooming_ and the _walking-on-the-side_ false positives.

TO-DO: incorporate the one-leg grooming (movie file: `11_6.avi`) and hyperactive fly (movie_file: `9_3.avi`, running around or walking on the side of the well, facing the camera sideways)
***

### 2022-08-10
> CRITICAL: Unique Identifiers should be used for subjects!

trained network-2 with extra video files (one-leg-grooming and hyperactive), the validation accuracy went down a bit (92.6%). More annotated videos will be required for the evaluation of the newly trained Network-2

### 2022-08-12
Hyperactivity seems to be no longer confused with grooming. Still have to test the one-leg grooming (the video I had was used for training, so it cannot be used for testing). The probability threshold seems to be in the range (0.5-0.75). I would use a value of 0.6, because any value higher seems to be increasing at a higher rate the number of false negatives as compared to the false positives.

### 2022-08-16
We decided to start from scratch: get rid of previous trained weights and use the full dataset to do a proper training/validation.
There will be 4 batches (2 temperature conditions by 2 laser power). 70% or 80% will be used for training + validation, the remaining will be used for test.

#### Plan for new training
- use network 1 as is (already trained weights)
- train on subset of the 4 batches (experiments), test on the remaining

### 2022-08-26

#### RESULTS ON TRAINING

#### Network-1
We used the same old weights for network 1 (trained on different  animals across different chamber orientations, camera not fixed, distinct resolutions).
- train set image size is (320 px, 256 px)
- number of test frames: 250
- pixel accuracy threshold: 8px, e.g. anything more distant than 8 pixels from the target is considered as an error. the head size is contained in a rectangle of ~10 px by ~5 px.
- accuracy: 98.80 %
- Test MEDIAN distance (+/- MAD)": 3.61 +/- 0.00 px,
- Test AVERAGE distance (+/- STD): 4.33 +/- 10.43 px

### Network-2

#### Training

Trained the network on a total of 22 subjects from 4 different experiments:
- 20 deg 6 mW: N=4
- 20 deg 10 mW: N=6
- 25 deg 6 mW: N=6
- 25 deg 10 mW: N=6

with the following number of frames:
- Training
- - Grooming (n=388)
- - NO Grooming (n=472)

- Validation
- - Grooming (n=98)
- - NO Grooming (n=119)

Overall Validation accuracy is 94.02 % 

#### TEST

- 24 animals

### 2022-08-30

#### Text for poster (NeuroFly 2022)
Network-1 (pytorch, 3 conv + 3 deconv layers) was trained/validated on low resolution images (320 x 256 px) and tested on a set of 250 images. the network was trained to identify the head of the fly (approximate head size: 10 x 5 px) by generating a per-pixel prediction map. The max value on the prediction map was defined as the center of the predicted position of the head . A correct classification was scored when the max value of the prediction map was within a radius of 8 pixels from the actual center of the head. Overall accuracy on the test set was 98.8% with a distance between actual and predicted position of 3.4 +/- 10.4 px (mean +/- sd).

Network-2 (Pytorch, pre-trained Resnet50) was trained/validated on 22 subjects from 4 different experiments. The network was trained/validated to recognize grooming or the absence of grooming (no_grooming) in single frames (projection of standard deviation across 3 frames). The total number of frames used were: training/grooming (n=388), training/no_grooming (n=472), validation/grooming (n=98), and validation/no_grooming (n=119). Overall accuracy on the validation set was 94.0%. We performed further analysis on the accuracy of the network's predictions on a separate test set (N=24 animals).

### 2022-09-27
Improved scripts for movie export and post-processing of raw data (output from two networks)

### 2022-11-18
Developed a new test script for network 2. it works directly on stack of frames (not movies). THis script assess the accuracy of network2 (taken in isolation).
The final accuracy is given by the composition of the two networks.
<mark>This means that the 'real' accuracy value is given by the application of the two networks (one after the other) on the full length movie</mark>

The pipeline is the following:
1. Process videos with network 1 to get head coordinates
2. Use the Ethovision file and head coordinates to extract and compose the stacks of frames.
3. Split frames into train/validation/test
4. Train/validate on train-validation set
5. Test on test set

Trained network 2 on 2 distinct datasets (1 regular experiment, 1 after injury)
- VALIDATION accuracy is 94.4 %
- TEST accuracy is ~91 % for `grooming` label and 97 % for `no_grooming label`

TO DO: test on full-length movie

### 2022-12-16
Trained two separate sets of weights: `before_injury` and `after_injury`.
> Validation accuracy is 95.9 % for `after_injury` and 96.1 for `before_injury`.
The idea is to swap the two sets of weights according to the type of experiment (before or after injury). As of today the accuracy of the network seems to be higher if training is kept separate.
I am waiting for the test set from Maria to evaluate accuracy on a test set.

### 2022-12-20
Must find a better way of scoring accuracy of Network1 (N1) + Network2 (N2).
Must use a rolling average (s=4) on grooming probability and then find the probability threhsold that maximizes the match between total count of manual grooming and total count of auto-grooming

Overall accuracy (i.e. total number of grooming_frames) is accurate, but the grooming episodes are not well aligned.
One possible solution is to :
1. get overall accuracy (i.e. compare total count of grooming_frames)
2. plot the inter-grooming interval.
High values in (1) and (2) should confirm that the network is doing a good job at detecting grooming (both in quantity and time).

AFTER INJURY: the overall accuracy is not very good when using the +10 convolution kernel on the grooming_frames. will try with the rolling average smoothing of the probability curve. I suspect that there aren't enough training samples. may add more sample from the before injury (I think this should not be detrimental)

### 2022-12-23
_EMAIL TO LUKAS AND MARIA_

I have placed the stack of grooming frames on the following NAS folder:
NAS:\RESEARCH\NEUROBAU\PUBLIC\LN\Maria\2022-12-23

I am also attaching a plot that shows the accuracy and error curve for the training on the BEFORE injury group.
I will use a different way of scoring the accuracy. you can find an excel file in the folder above with the data from the training that shows how the accuracy may be calculated.

It is too long to explain via email, but the gist of it is that we should smooth the grooming-probability curves, then look for the threshold that optimizes the overall grooming accuracy

as you can see in the excel file, the network during the training and validation does a pretty good job at guessing where the grooming episodes are located! But using the overall grooming accuracy does not let surface this event accuracy.
However, there can't be a frame-by-frame accuracy (i.e. the prediction is not well aligned to Maria's grooming in time).
so the conundrum is how to show that the network works properly?

something that comes to mind now that I am writing to you is to look at the inter-grooming interval and see if it correlates well with the manual scoring, this would reassure a potential reviewer about the fact that we use the overall grooming values as accuracy measure (and parameter to optimize the automatic detection).

I have attached a table that reports accuracies of the networks during training.
I know that the accuracy values are impressive, but you must keep in mind that the real value that must be reported (and that everyone cares) is the TEST accuracy value (this is what to expect when you will run the network "in the wild" during the screening). and this must be measured as a combined value for network1 and network2.

Also, I have created a folder with ALL the data (video, frames, manual scoring) used for this "network" project. this would make it easy to write paper + methods and eventually share the data.

### 2023-01-17
the AFTER injury on the test set (i.e. inference from network 1 + network 2) does not work.
Network2 detects grooming in animals that stand still (!?!?!?)
TO DO:
- check that the training set for AFTER injury is clean (i.e. see if frames of no_grooming have been erroneously included in the GROOMING label set)
- the AFTER training set is smaller than the BEFORE training set (~350 vs ~700). Try including extra frames from the BEFORE injury (preferentially add frames from the OLD experiments)
- Try to include extra animals (Maria has spare videos that could be used to create additional frames for the training set)

### 2023-01-24
there was an issue in the training code (AFTER and BEFORE weights were swapped)
Once the issue was fixed, the performance went back to nominal values.
the idea is to report the correlation value (during light onset) as accuracy measure for the final table (Network 1+2 assessment).

### 2023-02-02
the datasets have been renamed:
- `temperature-intensity` (uninjured flies)
- `after` (flies with antennae removed)
the idea is to use the train/validation sets to define a threshold for the grooming probability that is produced by network1 + network2.
the threshold should then take into account the cumulated errors from network 1 and network2 (trained on stack images, while during inference, the stacks are produced and evaluated by the network)
Once the threshold has been defined, then all the remaining plots can be produced on the basis of this threshold

### 2023-02-09
I have run the inference on multiple TRAIN sets (`temperature-intensity`, `after`). The results (table of auto grooming frames,target grooming during the light onset) have been collected in a single excel file.
it appears that the two sets have different thresholds (`temperature-intensity`: p>=0.3, `after`: p>=0.4). the idea is to minimize false positives (bias) even if the correlation is not optimal (?).

### 2023-02-13
the scripts have been glued together into two pipeline scripts:
- `run_networks.py` runs sequentially the two networks and that requires a "flag" (Before/after) to determine which checkpoint must be loaded.
- `data_analysis` consolidates the raw data (merges coordinates with probability of grooming) and generates an output table for both light ON and light OFF periods